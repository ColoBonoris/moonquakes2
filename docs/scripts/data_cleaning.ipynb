{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data from the raw csv's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and directions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# We'll have the datasets path here for easy access\n",
    "CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "DATASETS_DIR = os.path.join(Path(CURRENT_DIR).parent.parent, \"data\")\n",
    "\n",
    "FIRST_DATA_DIR = os.path.join(DATASETS_DIR, \"first\")\n",
    "REEDITION_DIR = os.path.join(DATASETS_DIR, \"reedition\")\n",
    "NEW_DATA_DIR = os.path.join(DATASETS_DIR, \"new\")\n",
    "\n",
    "# Then every dataset path will be defined here\n",
    "#   SM = shallow moonquake, DM = deep moonquake, AI = artificial impact, M = meteorite\n",
    "#   a = arrivals, l = locations\n",
    "#   n = new\n",
    "SMl_DIR = os.path.join(REEDITION_DIR, \"nakamura_1979_sm_locations.csv\")\n",
    "SMa_DIR = os.path.join(REEDITION_DIR, \"nakamura_1983_sm_arrivals.csv\")\n",
    "\n",
    "DMl_DIR = os.path.join(REEDITION_DIR, \"nakamura_2005_dm_locations.csv\")\n",
    "DMa_DIR = os.path.join(REEDITION_DIR, \"nakamura_2005_dm_arrivals.csv\")\n",
    "\n",
    "AIl_DIR = os.path.join(REEDITION_DIR, \"nakamura_1983_ai_locations.csv\")\n",
    "AIa_DIR = os.path.join(REEDITION_DIR, \"nakamura_1983_ai_arrivals.csv\")\n",
    "\n",
    "Ma_DIR = os.path.join(REEDITION_DIR, \"nakamura_1983_m_arrivals.csv\")\n",
    "\n",
    "L_DIR = os.path.join(REEDITION_DIR, \"lognonne_2003_catalog.csv\")\n",
    "\n",
    "DMn_DIR = os.path.join(NEW_DATA_DIR, \"Deep_Moonquakes_Areas.csv\")\n",
    "DMn_DIR_JSON = os.path.join(NEW_DATA_DIR, \"Deep_Moonquakes_Areas.json\")\n",
    "SMn_DIR = os.path.join(NEW_DATA_DIR, \"Shallow_Moonquakes_Areas.csv\")\n",
    "SMn_DIR_JSON = os.path.join(NEW_DATA_DIR, \"Shallow_Moonquakes_Areas.json\")\n",
    "Mn_DIR = os.path.join(NEW_DATA_DIR, \"Meteoroid_Impact_Areas.csv\")\n",
    "Mn_DIR_JSON = os.path.join(NEW_DATA_DIR, \"Meteoroid_Impact_Areas.json\")\n",
    "Ln_DIR = os.path.join(NEW_DATA_DIR, \"Catalogued_Events.csv\")\n",
    "Ln_DIR_JSON = os.path.join(NEW_DATA_DIR, \"Catalogued_Events.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Moonquakes (DM) Areas Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be used in the new dataset: \n",
    "    #   Area_ID (int),                                      -> Positive Integers\n",
    "    #   Longitude (grades), Longitude_Error (grades),       -> Float\n",
    "    #   Latitude (grades), Latitude_Error (grades),         -> Float\n",
    "    #   Depth (km), Depth_Error (km),                       -> Positive Integers\n",
    "    #   12_P_Mean (), 12_S_Mean (),                         -> Float\n",
    "    #   14_P_Mean (), 14_S_Mean (),                         -> Float\n",
    "    #   15_P_Mean (), 15_S_Mean (),                         -> Float\n",
    "    #   16_P_Mean (), 16_S_Mean (),                         -> Float\n",
    "\n",
    "new_columns = {\n",
    "    \"A\": \"Area_ID\",\n",
    "    \"Long\": \"Longitude\",\n",
    "    \"Long_err\": \"Longitude_Error\",\n",
    "    \"Lat\": \"Latitude\",\n",
    "    \"Lat_err\": \"Latitude_Error\",\n",
    "    \"Depth\" : \"Depth\",\n",
    "    \"Depth_err\": \"Depth_Error\",\n",
    "    \"12P\": \"12P_Mean\",\n",
    "    \"12S\": \"12S_Mean\",\n",
    "    \"14P\": \"14P_Mean\",\n",
    "    \"14S\": \"14S_Mean\",\n",
    "    \"15P\": \"15P_Mean\",\n",
    "    \"15S\": \"15S_Mean\",\n",
    "    \"16P\": \"16P_Mean\",\n",
    "    \"16S\": \"16S_Mean\",\n",
    "}\n",
    "\n",
    "columns_dtypes = {\n",
    "    \"Area_ID\": int,\n",
    "    \"Longitude\": float,\n",
    "    \"Longitude_Error\": float,\n",
    "    \"Latitude\": float,\n",
    "    \"Latitude_Error\": float,\n",
    "    \"Depth\": float,\n",
    "    \"Depth_Error\": float,\n",
    "    \"12P_Mean\": float,\n",
    "    \"12S_Mean\": float,\n",
    "    \"14P_Mean\": float,\n",
    "    \"14S_Mean\": float,\n",
    "    \"15P_Mean\": float,\n",
    "    \"15S_Mean\": float,\n",
    "    \"16P_Mean\": float,\n",
    "    \"16S_Mean\": float,\n",
    "}\n",
    "\n",
    "dml_df = pd.read_csv(DMl_DIR)\n",
    "dma_df = pd.read_csv(DMa_DIR)\n",
    "ndm_df = dml_df.copy()\n",
    "\n",
    "ndm_df = pd.merge(ndm_df, dma_df, how=\"outer\").sort_values(by=\"A\")\n",
    "\n",
    "ndm_df = (\n",
    "    ndm_df.rename(columns=new_columns)\n",
    "    .drop([\"Assumed\",\"Side\"], axis=1)\n",
    "    .replace([\"\",\" \"], 0).dropna()\n",
    "    .astype(columns_dtypes, copy=True)\n",
    ")\n",
    "\n",
    "ndm_df.to_csv(DMn_DIR,index=False,encoding=\"utf-8\")\n",
    "ndm_df.to_json(DMn_DIR_JSON,index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events (lognonne) Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from constants.dir import *\n",
    "import pandas as pd\n",
    "\n",
    "ldf = pd.read_csv(L_DIR)\n",
    "\n",
    "columns_renames = {\n",
    "    \"Type\": \"type\",\n",
    "    \"Latitude\": \"latitude\",\n",
    "    \"Longitude\": \"longitude\",\n",
    "    \"Depth\": \"depth\",\n",
    "    \"Delta-a\": \"delta_a\",\n",
    "    \"Delta-b\": \"delta_b\",\n",
    "    \"Phi\": \"phi\",\n",
    "    \"Depth_Error\": \"depth_error\",\n",
    "    \"Time_err\": \"time_error\",\n",
    "    \"Date\": \"date\",\n",
    "    \"12P\": \"p12\",\n",
    "    \"12S\": \"s12\",                   \n",
    "    \"14P\": \"p14\",\n",
    "    \"14S\": \"s14\",                   \n",
    "    \"15P\": \"p15\",\n",
    "    \"15S\": \"s15\",                   \n",
    "    \"16P\": \"p16\",\n",
    "    \"16S\": \"s16\",\n",
    "    \"12P_EC\": \"p12_ec\",\n",
    "    \"12S_EC\": \"s12_ec\",                   \n",
    "    \"14P_EC\": \"p14_ec\",\n",
    "    \"14S_EC\": \"s14_ec\",                   \n",
    "    \"15P_EC\": \"p15_ec\",\n",
    "    \"15S_EC\": \"s15_ec\",                   \n",
    "    \"16P_EC\": \"p16_ec\",\n",
    "    \"16S_EC\": \"s16_ec\"\n",
    "}\n",
    "\n",
    "columns_dtypes = {\n",
    "    \"Type\": str,\n",
    "    \"Latitude\": float,\n",
    "    \"Longitude\": float,\n",
    "    \"Depth\": float,\n",
    "    \"Delta-a\": float,\n",
    "    \"Delta-b\": float,\n",
    "    \"Phi\": float,\n",
    "    \"Depth_Error\": float,\n",
    "    \"Date\": \"datetime64[ns]\",\n",
    "    \"12P\": float,\n",
    "    \"12S\": float,                   \n",
    "    \"14P\": float,\n",
    "    \"14S\": float,                   \n",
    "    \"15P\": float,\n",
    "    \"15S\": float,                   \n",
    "    \"16P\": float,\n",
    "    \"16S\": float,\n",
    "    \"12P_EC\": float,\n",
    "    \"12S_EC\": float,                   \n",
    "    \"14P_EC\": float,\n",
    "    \"14S_EC\": float,                   \n",
    "    \"15P_EC\": float,\n",
    "    \"15S_EC\": float,                   \n",
    "    \"16P_EC\": float,\n",
    "    \"16S_EC\": float\n",
    "}\n",
    "\n",
    "sepparate_dates = (lambda x: f\"{x[0:2]}-{x[2:4]}-{x[4:6]} {x[6:8]}:{x[8:]}\")\n",
    "\n",
    "# Getting rid out of / fixing detected and documented errors\n",
    "ldf = ldf[ldf[\"Date\"] > 1000000000]\n",
    "\n",
    "\n",
    "ldf[\"Date\"] = ldf[\"Date\"].astype(str).apply(sepparate_dates)\n",
    "ldf[\"Seconds\"] = ldf[\"Seconds\"].astype(int).astype(str).apply(lambda x: f\"{x:0>2}\")\n",
    "ldf[\"Date\"] = \"19\" + ldf[\"Date\"] + \":\" + ldf[\"Seconds\"]\n",
    "ldf[\"Date\"] = (pd.to_datetime(ldf[\"Date\"], format=\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "ldf = (\n",
    "    ldf.sort_values(by=\"Date\")\n",
    "    .rename(columns={\"Long\":\"Longitude\", \"Lat\":\"Latitude\", \"Depth_err\": \"Depth_Error\"})\n",
    "    .astype(columns_dtypes)\n",
    ")\n",
    "ldf = ldf.fillna(0)\n",
    "ldf = ldf.rename(columns=columns_renames).drop([\"Seconds\"], axis=1)\n",
    "\n",
    "ldf.to_csv(Ln_DIR,index=False,encoding=\"utf-8\")\n",
    "ldf.to_json(Ln_DIR_JSON, indent=2, orient=\"table\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
